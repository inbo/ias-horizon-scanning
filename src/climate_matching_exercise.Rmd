---
title: "climate_matching_exercise"
author: "Soria Delva"
date: "2024-09-24"
output: html_document
---

##Setup
Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries.

```{r Load libraries}
library(dplyr)
library(rgbif)
library(progress)
library(sf)
library(rnaturalearth)
library(terra)
library(data.table)
library(mapview)
library(here)
library(stringr)
library(CoordinateCleaner)
library(progress)
```

## Background information

For the climate matching exercise, we will use the specieskeys that were processed in the 50km_grid exercise.
Specifically, longlist species were matched with the GBIF backbone. Species for which no match was found were corrected and rematched with the backbone. Synonyms were converted to an accepted taxon and varieties and subspecies were scaled up to the species level. This list was used to perform a download of occurrence records, with the following filters:
A temporal filter: 1980-2024
A spatial filter: occurrences within EU member states and their EEZ
A basis of record filter: only keeping records of the types "OBSERVATION", "HUMAN_OBSERVATION", "MATERIAL_SAMPLE", "LITERATURE", "PRESERVED_SPECIMEN",  "UNKNOWN", "MACHINE_OBSERVATION". We don't take into account `FOSSIL SPECIMEN` and `LIVING SPECIMEN`, which can have misleading location information.
A filter on occurrence types: only presences are kept with associated coordinates

These downloaded records were further cleaned, removing (0,0) coordinates, records around known biodiversity institutions and the GBIF headquarters in Copenhagen, records around capitals and country centroids, and records with a coordinateUncertainty higher than 50 km.

From the 7041 species that were included in the download, only x remain after the indicated filtering steps. 
For each of these species, we calculated the number of 50km x 50km grids in EU member states and their EEZ that were occupied, where a grid is indicated as occupied when it holds observations in at least two different years.The species that only occupied a maximum of two grids were kept (n = ) and combined with those that were removed in the download or subsequent filtering steps (n = 4494), yielding a total number of 5054 species. From those, we removed marine species as we don't have climatic data for those, yielding a total of 4584 species for climate matching.


##Global data download

Global occurrence data for each of these species are downloaded.

```{r}
#Extract taxonkeys as vector (length 4584)
taxonkeys<-climate_matching%>%
  pull(speciesKey)

#Indicate basis of record for download
basis_of_record <- c(
      "OBSERVATION", 
      "HUMAN_OBSERVATION",
      "MATERIAL_SAMPLE", 
      "LITERATURE", 
      "PRESERVED_SPECIMEN", 
      "UNKNOWN", 
      "MACHINE_OBSERVATION")

# Identification_verification_status to discard
identificationverificationstatus_to_discard<- c(
      "unverified",
      "unvalidated",
      "not validated",
      "under validation",
      "not able to validate",
      "control could not be conclusive due to insufficient knowledge",
      "Control could not be conclusive due to insufficient knowledge",
      "1",
      "uncertain",
      "unconfirmed",
      "Douteux",
      "Invalide",
      "Non r\u00E9alisable",
      "verification needed" ,
      "Probable",
      "unconfirmed - not reviewed",
      "validation requested"
    )



gbif_download <- occ_download(
      pred_in("taxonKey", taxonkeys),
      pred("hasGeospatialIssue", FALSE), #Remove default geospatial issues
      pred("hasCoordinate", TRUE), # Keep only records with coordinates
      pred("occurrenceStatus", "PRESENT"), 
      pred_in("basisOfRecord", basis_of_record),
      pred_gt("year", 1900),
      pred_not(pred_in("identificationVerificationStatus",identificationverificationstatus_to_discard)),
      pred_or(  
        pred_lt("coordinateUncertaintyInMeters",50000),
        pred_isnull("coordinateUncertaintyInMeters")
        ),
      format="SIMPLE_CSV"
      )
 
  #Follow the status of the download
occ_download_wait(gbif_download)

  
 #Retrieve downloaded records: 
 #gbif_download<-"0028120-240906103802322"
download_path <-occ_download_get(gbif_download, overwrite = TRUE, path=tempdir()) %>%
    unzip( exdir = tempdir())

cm_records<-data.table::fread(file.path(tempdir(),paste0(gbif_download,".csv")),
                              select=c("speciesKey","scientificName","year","decimalLatitude", "decimalLongitude", "coordinateUncertaintyInMeters", "kingdom","phylum","class","order", "family", "genus", "taxonKey"), nrow=10000000)

#Check how many speciesKeys are present in download
length(unique(cm_records$speciesKey)) #4312

#Retrieve citation of downloaded dataset
print(gbif_citation(occ_download_meta(gbif_download))$download)

 
```

##Data preparation

Filter data based on coordinate uncertainty, 

```{r Data preparation}

cm_records <- cm_records %>%
      #cc_cen() %>% # remove country centroids 
      #cc_cap() %>% # remove capitals centroids
      #cc_inst() %>% # remove zoo and herbaria
      #cc_gbif()%>%
      #cc_zero()%>%
      select(c("year",
             "speciesKey",
             "decimalLatitude",
             "decimalLongitude",
             "coordinateUncertaintyInMeters"
             )) %>%
      mutate(year_cat = dplyr::case_when(year <= 1925 ~ "1901-1925",
                                                year <= 1950 ~ "1926-1950",
                                                year <= 1975 ~ "1951-1975",
                                                year <= 2000 ~ "1976-2000",
                                                year > 2000 ~ "2001-2025",
                                                TRUE ~ NA_character_)) %>%
      dplyr::group_by(year_cat,speciesKey, decimalLatitude,decimalLongitude) %>%
      dplyr::mutate(coordinateUncertaintyInMeters = ifelse(all(is.na(.data$coordinateUncertaintyInMeters)),
                                                           NA_real_,
                                                           min(.data$coordinateUncertaintyInMeters, na.rm = TRUE)))%>%
      dplyr::summarize(n_obs = dplyr::n(),
                       coordinateUncertaintyInMeters = dplyr::first(.data$coordinateUncertaintyInMeters)) %>%
      dplyr::ungroup() 
 

#Convert to sf dataframe 
cm_records <- cm_records %>% 
    sf::st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326, remove = FALSE)

```



##Climate matching

Link each occurrence point to the climatic region and corresponding gridcode at the time of its observation

```{r}
# Load climate layers
load("./data/input/climate_matching/future.rda")
load("./data/input/climate_matching/legends.rda")
load("./data/input/climate_matching/observed.rda")

# Import legends
KG_Rubel_Kotteks_Legend <- legends$KG_A1FI
KG_Beck <- legends$KG_Beck

# Define time periods
timeperiodes <- c("1901-1925", 
                  "1926-1950",
                  "1951-1975",
                  "1976-2000",
                  "2001-2025")

#Initiate empty dataframe
cm_current <- data.frame()
  
for(t in timeperiodes){
    
  print(paste0("Time period: ", t))
  
  # Determine subset parameters    
  start <- as.numeric(substr(t, 0, 4))
    
  # Subset spatial data
  data_sf_sub <- cm_records%>% 
      dplyr::filter(.data$year_cat == t)
    
  print(paste0("Number of observations: ", nrow(data_sf_sub)))
    
  
  if(nrow(data_sf_sub)>0){
    
      #Load appropriate climate layer  
      if(start <= 2000){
          obs_shape <- observed[[t]]
         }else{
          t <- "2001-2025-A1FI"
          obs_shape <- future[[t]]
         }
      
      #Add GRIDCODE and information of climatic region to each observation 
      data_sf_sub<- st_join(data_sf_sub, obs_shape, join = st_within, left=FALSE)%>%
        mutate(GRIDCODE= as.double(GRIDCODE))%>%
        left_join(KG_Rubel_Kotteks_Legend, by = c("GRIDCODE")) 
        
      
      #Combine these data in a data frame
        if(nrow(cm_current)==0){
          cm_current<-data_sf_sub
        }else{
          cm_current<-rbind(cm_current, data_sf_sub)
        }
        
    }else {
      warning(paste0("No data was present in the GBIF dataset for ", t))
      next
    }

}

  # Cleanup
  remove(obs_shape, data_sf_sub)
  
  #Drop geometry column
  cm_current<-sf::st_drop_geometry(cm_current)
  
  #Calculate percentage climate matching
  cm_current<- cm_current %>% 
    group_by(speciesKey, Classification) %>% 
    mutate(n_climate = sum(n_obs, na.rm = TRUE)) %>% 
    ungroup() %>% 
    group_by(speciesKey) %>% 
    mutate(n_totaal = sum(n_obs, na.rm = TRUE)) %>% 
    ungroup() %>% 
    mutate(perc_climate = (n_climate/n_totaal)*100) %>% 
    distinct(speciesKey, Classification,
                    .keep_all = TRUE)  %>% 
    select("speciesKey", 
           "Classification", 
           "Description",
           "n_climate", 
           "n_totaal", 
           "perc_climate")
  
  
```

Create a dataframe with the climate regions, their gridcodes and corresponding information that occur in the region of interest under current and each future scenario 

```{r Calculate climate matching percentage in EU member states}

# Determine current and nearest future scenarios 
scenarios <- c("2001-2025-A1FI",
               "2026-2050-A1FI")

#Get a shapefile with EU member states
memberstates<-read.csv2(here("./data/input/GRIIS_checklists.csv"))%>%
  pull(EU_state)

region_shape<-st_read("./data/spatial/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp")%>%
  filter(ADMIN%in%memberstates)%>%
  st_crop(xmin=-13, ymin=20, xmax=41, ymax= 81)%>%
  select(c(ADMIN, geometry))

st_agr(region_shape) = "constant"

# Create empty future_climate 
future_climate <- data.frame() %>% 
  mutate(scenario = "",
         KG_GridCode = as.integer(""))
  
# Calculate KG codes 
for (s in scenarios) {
  
  print(paste0("Time period: ", s))
  
  #Extract Köppen Geiger layer for the selected future scenario
  shape <- future[[s]]%>% 
    st_set_crs(4326)
  
  #Make sure column gridcode is indicated in capital letters and of class integer
  if (c("gridcode") %in% colnames(shape)) {
    shape <- shape %>% 
      rename(GRIDCODE = "gridcode")%>%
      mutate(GRIDCODE = as.integer(GRIDCODE))
  }else{
    shape<- shape%>%
      mutate(GRIDCODE = as.integer(GRIDCODE))
  }
    
    
  #Set attributes of shapefile (data columns) to constant throughout the geometry to avoid warning   during st_intersection
  st_agr(shape) = "constant"
    
  #Intersect the Köppen Geiger layer with the region of interest (EU member states)
  gridcode_intersect<-st_intersection(shape, region_shape)
    
  #Create an future_climate dataframe holding the gridcodes of the Köppen Geiger zones present in the region of interest and the associated future scenario 
  for (g in gridcode_intersect$GRIDCODE) {
    future_climate <- future_climate %>% 
              add_row(scenario = s,
                      KG_GridCode = g)
    }
  }
  
  
future_climate <- future_climate%>%
  left_join(y = KG_Rubel_Kotteks_Legend, by = c("KG_GridCode" = "GRIDCODE"))%>%
  filter(!is.na(.data$Classification))


  
###Link the dataframe with the climatic regions  under all future scenarios to the % climate match data
cm <- data.frame()
  
for (b in unique(future_climate$scenario)) {
    future_scenario <- future_climate %>% 
       filter(scenario == b)
    
    cm_int <- cm_current %>% 
      filter(Classification %in% future_scenario$Classification
      ) %>% 
      mutate(scenario = b)
    
    if (nrow(cm) == 0) {
      cm <- cm_int
    } else {
      cm <- rbind(cm, cm_int)
    }
  }
  
remove (cm_int)

#----Set thresholds-----

#Number of occurrences used to calculate climate matching
n_limit <- 0

#Minimum percent of climate matching
cm_limit <- 0
  
  
cm<- cm %>% 
    filter(n_totaal >= n_limit,
                  perc_climate >= cm_limit)
```

